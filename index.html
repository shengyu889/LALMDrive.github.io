<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Foundation Model for Low-Altitude Coordinated Autonomous Driving - Anonymous CVPR Submission">
  <meta name="description" content="We propose LALMDriver, an end-to-end autonomous driving framework integrating Vision-Language Models with UAV aerial perspectives for enhanced reasoning and safety.">
  <meta name="keywords" content="autonomous driving, VLM, UAV, low-altitude, collaborative intelligence, chain-of-thought, LLaVA, CARLA, multimodal reasoning, end-to-end driving">
  <meta name="author" content="Anonymous Authors">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="LALMDriver Project">
  <meta property="og:title" content="Foundation Model for Low-Altitude Coordinated Autonomous Driving">
  <meta property="og:description" content="An end-to-end autonomous driving framework that integrates Vision-Language Models with UAV aerial perspectives to enhance decision-making, spatial reasoning, and trajectory planning in complex environments.">
  <meta property="og:url" content="https://shengyu889.github.io/LALMDrive.github.io/">
  <meta property="og:image" content="https://shengyu889.github.io/LALMDrive.github.io/static/images/pipeline.jpg">
  <meta property="og:image:width" content="480">
  <meta property="og:image:height" content="200">
  <meta property="og:image:alt" content="LALMDriver - Low-Altitude Collaborative Autonomous Driving Framework">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Anonymous Authors">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="autonomous driving">
  <meta property="article:tag" content="vision-language models">
  <meta property="article:tag" content="UAV collaboration">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@LALMDriver">
  <meta name="twitter:creator" content="@LALMDriver">
  <meta name="twitter:title" content="Foundation Model for Low-Altitude Coordinated Autonomous Driving">
  <meta name="twitter:description" content="LALMDriver: VLM-based end-to-end autonomous driving with UAV aerial perspective integration for enhanced reasoning and safety.">
  <meta name="twitter:image" content="https://shengyu889.github.io/LALMDrive.github.io/static/images/pipeline.jpg">
  <meta name="twitter:image:alt" content="LALMDriver Framework Pipeline">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Foundation Model for Low-Altitude Coordinated Autonomous Driving">
  <meta name="citation_author" content="Anonymous, Authors">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CVPR 2026">
  <meta name="citation_pdf_url" content="https://shengyu889.github.io/LALMDrive.github.io/static/pdfs/Foundation Model for Low-Altitude Coordinated Autonomous Driving.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <!-- Title -->
  <title>Foundation Model for Low-Altitude Coordinated Autonomous Driving | LALMDriver</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Foundation Model for Low-Altitude Coordinated Autonomous Driving",
    "description": "An end-to-end autonomous driving framework that leverages Vision-Language Models with UAV aerial perspectives to enhance decision-making, spatial reasoning, and trajectory planning in complex environments.",
    "author": [
      {
        "@type": "Person",
        "name": "Anonymous Authors"
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CVPR 2026"
    },
    "url": "https://lalmdriver.github.io",
    "image": "https://lalmdriver.github.io/static/images/social_preview.png",
    "keywords": ["autonomous driving", "vision-language models", "UAV", "chain-of-thought", "CARLA"],
    "abstract": "The integration of Multimodal Large Language Models (MLLMs) into autonomous driving (AD) systems represents a transformative leap in perception and reasoning. This paper introduces an end-to-end autonomous driving framework for low-altitude collaborative intelligence, leveraging Vision-Language Models (VLMs) to enhance decision-making, spatial reasoning, and trajectory planning. Our approach uniquely incorporates a drone perspective into the reasoning chain, expanding situational awareness beyond ground-level blind spots and enabling proactive risk avoidance. The drone component may function as either vehicle-mounted or standalone, providing overhead context for dynamic traffic scenes. The model generates comprehensive scene descriptions, identifies critical entities, and infers potential risks by integrating historical context and ego-state information. A closed-loop evaluation on CARLA demonstrates that our method not only improves trajectory accuracy but also significantly enhances robustness in complex and dynamic environments.",
    "citation": "@article{LALMDriver2024,\n  title={Foundation Model for Low-Altitude Coordinated Autonomous Driving},\n  author={Anonymous Authors},\n  journal={CVPR},\n  year={2024},\n  url={https://lalmdriver.github.io}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://lalmdriver.github.io"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Autonomous Driving"
      },
      {
        "@type": "Thing", 
        "name": "Vision-Language Models"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "LALMDriver Research Project",
    "url": "https://lalmdriver.github.io",
    "logo": "https://lalmdriver.github.io/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/LALMDriver",
      "https://github.com/lalmdriver"
    ]
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View Related Works">
      <i class="fas fa-flask"></i>
      Related Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Related Works in Autonomous Driving</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://arxiv.org/abs/2402.12289" class="work-item" target="_blank">
          <div class="work-info">
            <h5>DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models</h5>
            <p>Explores VLM integration for interpretable autonomous driving with chain-of-thought reasoning.</p>
            <span class="work-venue">arXiv 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/2312.14150" class="work-item" target="_blank">
          <div class="work-info">
            <h5>DriveLM: Driving with Graph Visual Question Answering</h5>
            <p>Graph-based VQA approach for structured driving reasoning and decision-making.</p>
            <span class="work-venue">arXiv 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/2408.09251" class="work-item" target="_blank">
          <div class="work-info">
            <h5>V2X-VLM: End-to-End V2X Cooperative Autonomous Driving</h5>
            <p>Explores vehicle-to-everything cooperation using large vision-language models.</p>
            <span class="work-venue">arXiv 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/2303.08774" class="work-item" target="_blank">
          <div class="work-info">
            <h5>LMDrive: Closed-Loop End-to-End Driving with Large Language Models</h5>
            <p>Baseline method for language model-based closed-loop autonomous driving.</p>
            <span class="work-venue">CVPR 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Foundation Model for Low-Altitude Coordinated Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://anonymous.github.io" target="_blank">Anonymous Author<sup>*</sup></a>
              </span>
              <span class="author-block">
                <a href="https://anonymous.github.io" target="_blank">Anonymous Author<sup>*</sup></a>
              </span>
              <span class="author-block">
                <a href="https://anonymous.github.io" target="_blank">Anonymous Author</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Institution<br>CVPR 2026 Submission #20277</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/paper.pdf" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/anonymous/lalmdriver" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.00000" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
       
          <img src="static/images/pipeline.jpg" alt="LALMDriver Framework Pipeline" loading="lazy"/>
         
        <h2 class="subtitle has-text-centered">
          LALMDriver framework integrating ground-view and UAV aerial perspectives with Vision-Language Model reasoning for enhanced autonomous driving in occluded and complex scenarios.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The integration of Multimodal Large Language Models (MLLMs) into autonomous driving (AD) systems represents a transformative leap in perception and reasoning. This paper introduces an end-to-end autonomous driving framework for low-altitude collaborative intelligence, leveraging Vision-Language Models (VLMs) to enhance decision-making, spatial reasoning, and trajectory planning. Our approach uniquely incorporates a drone perspective into the reasoning chain, expanding situational awareness beyond ground-level blind spots and enabling proactive risk avoidance. The drone component may function as either vehicle-mounted or standalone, providing overhead context for dynamic traffic scenes. The model generates comprehensive scene descriptions, identifies critical entities, and infers potential risks by integrating historical context and ego-state information. A closed-loop evaluation on CARLA demonstrates that our method not only improves trajectory accuracy but also significantly enhances robustness in complex and dynamic environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Key Contributions -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Key Contributions</h2>
          <div class="content has-text-justified">
            <ul>
              <li>A <strong>low-altitude cooperative E2E framework</strong> for autonomous driving that leverages VLMs' world knowledge and reasoning capability to improve interpretability, safety, and generalization beyond traditional rule-based AD systems.</li>
              <li>A <strong>novel VQA-CoT dataset</strong> incorporating UAV perspectives into the reasoning process, enhancing the vehicle's understanding of occluded and dynamic regions.</li>
              <li><strong>LoRA-based fine-tuning</strong> of VLM backbones (LLaVA-v1.6) for structured reasoning and trajectory generation, with superior closed-loop performance over LMDrive in complex CARLA scenarios.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/pipeline.jpg" alt="LALMDriver Framework Pipeline" style="width: 1000; height: 600;”loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Figure 1: The pipeline of our LALMDriver. The low-altitude collaborative end-to-end model based on VLM aims to provide comprehensive road information, build urban agents, and improve interpretability.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/contrast2.jpg" alt="Motivation and Key Highlights" style="width: 800; height: 500;”loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Figure 2: Illustration of the motivation and key highlights. Our method combines instructions with visual inputs from vehicle front views and UAV perspectives to improve reasoning consistency and safety compared to baseline VLM and VLM+CoT approaches.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/data.jpg" alt="Chain of Thought Process" style="width: 1000; height: 600;”loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Figure 3: Presentation of the chain of thought when building data. The model accumulates understanding and reasoning context of the entire driving scene through structured Q&A templates.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/spawn_data.jpg" alt="Automated Annotation Pipeline" style="width: 800; height: 500;loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Figure 4: The automated annotation pipeline using Kimi-1.5-671B model to generate structured reasoning annotations including scene descriptions, object recognition, intention prediction, and driving actions.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/scenario.jpg" alt="Qualitative Results" style="width: 800; height:500;”loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Figure 5: Qualitative results showing the model's driving action reasoning and trajectory prediction outputs in closed-loop evaluation scenarios.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <!-- Method Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Method Overview</h2>
          <div class="content has-text-justified">
            <p>
              Our framework synergistically integrates multimodal VLM reasoning with aerial-ground perception fusion for closed-loop end-to-end driving. We adopt <strong>LLaVA-v1.6-7B</strong> as the vision-language backbone due to its strong cross-modal comprehension and open-source adaptability. The model takes as input front-view vehicle images, multi-angle UAV frames, navigation instructions, and ego-state information to generate hierarchical planning outputs including meta-actions, trajectory waypoints, and reasoning explanations.
            </p>
            <p>
              The training consists of two stages: (1) CoT fine-tuning with LoRA to enhance reasoning ability and interpretability, and (2) planner training with L1 waypoint loss and classification loss for instruction completion. This approach enables precise vehicle control through PID controllers while maintaining interpretable decision-making.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Experimental Results -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Experimental Results</h2>
          <div class="content has-text-justified">
            <p>
              We evaluate our framework through closed-loop simulations in CARLA, focusing on both custom low-altitude cooperation scenarios and the Town05-Long benchmark. LALMDriver achieves a <strong>Driving Score (DS) of 58.3</strong>, surpassing previous closed-loop E2E methods including LMDrive (57.2 DS). The improvements stem from enhanced reasoning consistency and global situational awareness brought by the UAV-assisted CoT mechanism.
            </p>
            <p>
              In multi-ability evaluation across four complex scenarios, LALMDriver achieves the lowest failure rates: 0.00% in Pedestrian Crossing, 0.09% in Overtaking with Occlusion, 0.03% in Intersection with Hidden Vehicles, and 0.06% in Detour after Accident. These results highlight the model's strong robustness and capability to generalize across occluded, multi-agent, and long-range reasoning conditions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- Video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Scenario Demonstrations</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="static/videos/poster1.jpg" id="video1" controls muted loop height="100%" preload="metadata">
              <source src="static/videos/demo1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="static/videos/poster2.jpg" id="video2" controls muted loop height="100%" preload="metadata">
              <source src="static/videos/demo2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="static/videos/poster3.jpg" id="video3" controls muted loop height="100%" preload="metadata">
              <source src="static/videos/demo3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video4">
            <video poster="static/videos/poster3.jpg" id="video4" controls muted loop height="100%" preload="metadata">
              <source src="static/videos/demo4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->

  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Poster</h2>
        <iframe src="static/pdfs/Foundation_Model_for_Low_Altitude_Coordinated_Autonomous_Driving.pdf" width="100%" height="650">
        </iframe>
      </div>
    </div>
  </section>
  <!--End paper poster -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{LALMDriver2024,
  title={Foundation Model for Low-Altitude Coordinated Autonomous Driving},
  author={Anonymous Authors},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025},
  url={https://shengyu889.github.io/LALMDrive.github.io/}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  <!-- End of Statcounter Code -->

  </body>
  </html>
